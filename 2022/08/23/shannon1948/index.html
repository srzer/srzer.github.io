<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5"><title>Ruizhe Shi</title><meta name="author" content="Ruizhe Shi"><link rel="shortcut icon" href="/img/favicon.png"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.13.0/css/all.min.css"><meta name="generator" content="Hexo 6.3.0"></head><body><header id="page_header"><div class="header_wrap"><div id="blog_name"><a class="blog_title" id="site-name" href="/">Ruizhe Shi</a></div><button class="menus_icon"><div class="navicon"></div></button><ul class="menus_items"><li class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https://scholar.google.com/citations?user=0tlXSPkAAAAJ&amp;hl=zh-CN"> Publications</a></li><li class="menus_item"><a class="site-page" href="/blogs"> Blogs (in Chinese)</a></li></ul></div></header><main id="page_main"><div class="side-card sticky"><div class="card-wrap" itemscope itemtype="http://schema.org/Person"><div class="author-avatar"><img class="avatar-img" src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/profile.png'" alt="avatar"></div><div class="author-discrip"><h3>Ruizhe Shi</h3><p class="author-bio">我们从坚果剥出时间并教它走路.</p></div><div class="author-links"><button class="btn m-social-links">Links</button><ul class="social-icons"><li><a class="social-icon" href="https://twitter.com/smellycat_ZZZ" target="_blank"><i class="fab fa-twitter" aria-hidden="true"></i></a></li><li><a class="social-icon" href="https://github.com/srzer" target="_blank"><i class="fab fa-github" aria-hidden="true"></i></a></li><li><a class="social-icon" href="mailto:srz21 at mails dot tsinghua dot edu dot cn" target="_blank"><i class="fas fa-envelope" aria-hidden="true"></i></a></li></ul></div></div></div><div class="page" itemscope itemtype="http://schema.org/CreativeWork"><h2 class="page-title">shannon1948</h2><article><p>The fundamental problem of communication is that of <strong>reproducing at one point either exactly or approximately a message selected at another point</strong>. </p>
<p>The significant aspect is that <strong>the actual message is one <em>selected from a set</em> of possible messages</strong>. The system must be designed to operate for each possible selection, not just the one which will actually be chosen since this is unknown at the time of design.</p>
<p>By a communication system we will mean a system of the type indicated schematically, it consists of essentially five parts:</p>
<ol>
<li><p>information source</p>
</li>
<li><p>transmitter</p>
<p>operates on the message in some way to produce a signal suitable for transmission over the channel</p>
</li>
<li><p>channel</p>
<p> the medium used to transmit the signal from transmitter to receiver</p>
</li>
<li><p>receiver</p>
<p>performs the inverse operation of that done by the transmitter</p>
</li>
<li><p>destination</p>
</li>
</ol>
<p><strong>3 main categories:</strong></p>
<p>discrete, continuous and mixed.</p>
<h2 id="Discrete"><a href="#Discrete" class="headerlink" title="Discrete"></a>Discrete</h2><h3 id="The-discrete-noise-channel"><a href="#The-discrete-noise-channel" class="headerlink" title="The discrete noise channel"></a>The discrete noise channel</h3><p>Shannon在这里对最简单离散情况分析了频道的容量问题，并且说明了信息量的对数随时间线性增加。对于不同符号的时长不同的情况，Shannon说明了频道的容量与特征方程的最大实数根的对数相关。</p>
<p><strong>The capacity C of a discrete channel</strong><br>$$<br>C&#x3D;\lim_{T\rightarrow \infty}\frac{\log N(T)}{T}<br>$$<br>where $N(T)$ is the number of allowed signals of duration $T$.</p>
<p>Suppose all sequences of the symbols $S_1,…,S_n$ are allowed and these symbols have durations $t_1,…,t_n$. Then we have<br>$$<br>N(t)&#x3D;N(t-t_1)+…+N(t-t_n)<br>$$<br>The characteristic equation is<br>$$<br>X^{-t_1}+X^{-t_2}+…+X^{-t_n}&#x3D;1<br>$$<br>Let $X_0$ be the largest real solution of the characteristic equation. Then $N(t)\sim X_0^t$.<br>$$<br>C&#x3D;\log X_0<br>$$<br><strong>Restriction: For each state only certain symbols from the can be transmitted</strong></p>
<p><strong>Theorem 1</strong></p>
<p>Let $b_{ij}^{(s)}$ be the duration of the $s^{th}$ symbol which is allowable in state $i$ and leads to state $j$.</p>
<p>Then the channel capacity $C$ is equal to $\log W$ where $W$ is the largest real root of the determinant equation:<br>$$<br>|\sum_sW^{-b_{ij}^{(s)}}-\delta_{ij}|&#x3D;0<br>$$<br>where $\delta_{ij}&#x3D; (i&#x3D;&#x3D;j)$</p>
<h3 id="The-discrete-source-of-information"><a href="#The-discrete-source-of-information" class="headerlink" title="The discrete source of information"></a>The discrete source of information</h3><p>Shannon提出了随机过程与离散源的对等。并对离散源进行了分类。</p>
<p>A physical system, or a mathematical model of a system which produces such a sequence of symbols governed by a set of probabilities, is known as a stochastic process.</p>
<p> Conversely, any stochastic process which produces a discrete sequence of symbols chosen from a finite set may be considered a discrete source.</p>
<ol>
<li>自然语言</li>
<li>连续源经过量化</li>
<li>数学例子</li>
</ol>
<h3 id="The-series-of-approximation"><a href="#The-series-of-approximation" class="headerlink" title="The series of approximation"></a>The series of approximation</h3><p>描述了对于语言的近似方法，零阶近似（等概率任取），一阶（保持概率分布与自然语言相等），二阶（考虑前缀。。。）举了很多近似的例子后，他说，“It appears then that a sufficiently complex stochastic process will give a satisfactory representation of a discrete source.”</p>
<h3 id="各态历经混合源"><a href="#各态历经混合源" class="headerlink" title="各态历经混合源"></a>各态历经混合源</h3><p>Shannon给出各态历经(ergodic)的条件判定</p>
<ol>
<li>连通</li>
<li>circuit的长度的gcd为1，否则周期化</li>
</ol>
<p>equilibrium conditions:<br>$$<br>P_j&#x3D;\sum_{i}P_ip_i(j)<br>$$<br>In the ergodic case it can be shown that with any starting conditions the probabilities $P_j(N)$ of being in state $j$ after $N$ symbols, approach the equilibrium values as $N\rightarrow \infty$.</p>
<h3 id="Choice-uncertainty-and-entropy"><a href="#Choice-uncertainty-and-entropy" class="headerlink" title="Choice, uncertainty and entropy"></a>Choice, uncertainty and entropy</h3><p>Shannon在说明了离散信息源实质上是马尔科夫过程后，他想要导出，如何衡量产生信息的速度？</p>
<p>在已知即将发生的事件的概率的情况下，我们对结果有多大的不确定性？</p>
<p>我们来推导$H(p_i)$</p>
<p>$H(p_1,…,p_n)&#x3D;H(p_1,1-p_1)+(1-p_1)H(p_2,…,p_n)$</p>
<p>$H(\frac{1}{pq},…,\frac{1}{pq})&#x3D;H(\frac{1}{p},…,\frac{1}{p})+H(\frac{1}{q},…,\frac{1}{q})$</p>
<p>所以$H(\frac{1}{n},…,\frac{1}{n})&#x3D;K\ln n$</p>
<p>注意到一个事件产生的不确定性和其他分支没有关系</p>
<p>所以我们可以认为$p_i$的贡献是$-Kp_i\ln p_i$</p>
<p>一种表达形式：<br>$$<br>H(x,y)&#x3D;-\sum_{i,j}p(i,j)\log p(i,j)\<br>H(x)&#x3D;-\sum_{i,j}p(i,j)\log\sum_{j}p(i,j)\<br>H(y)&#x3D;-\sum_{i,j}p(i,j)\log\sum_{i}p(i,j)<br>$$<br>我们有不等式<br>$$<br>H(x,y)\le H(x)+H(y)\<br>这是怎么证的呢<br>$$<br>取等为前后两次独立。</p>
<p>$H$表现为概率越均等则越大的趋势<br>$$<br>-\sum_{i,j}p(i,j)\log p(i,j)\le -\sum_i p(i)\log p(i)-\sum_{j}p(j)\log p(j)<br>$$</p>
<p>$$<br>p_i(j)&#x3D;\frac{p(i,j)}{\sum_j p(i,j)}<br>$$</p>
<p>$$<br>H_x(y)&#x3D;-\sum_{i,j}p(i,j)\log p_i(j)<br>$$</p>
<p>这衡量了当我们知道x的时候对于y的不确定性<br>$$<br>H_x(y)&#x3D;-\sum_{i,j}p(i,j)\log p(i,j)+\sum_{i,j}p(i,j)\log \sum_{j}p(i,j)\<br>&#x3D;H(x,y)-H(x)<br>$$</p>
<p>$$<br>H(y)\ge H_x(y)<br>$$</p>
<h3 id="The-entropy-of-an-information-source"><a href="#The-entropy-of-an-information-source" class="headerlink" title="The entropy of an information source"></a>The entropy of an information source</h3><p>源的熵是状态的熵的加权平均<br>$$<br>H&#x3D;\sum_iP_iH_i\<br>H’&#x3D;\sum_i f_iH_i(per second)\<br>H’&#x3D;mH<br>$$<br>If symbols are independent, then $H$ is simply $-\sum p_i\log p_i$</p>
<p>Then consider a long message of $N$ symbols<br>$$<br>p&#x3D;\prod_{i&#x3D;1}^np_i^{p_iN}<br>$$</p>
<p>$$<br>\log p &#x3D; N\sum_{i}p_i\log p_i\&#x3D;-NH<br>$$</p>
<p><strong>Theorem 3</strong></p>
<p>Given any $\epsilon&gt;0$ and $\delta &gt;0$, we can find an $N_0$ such that the sequences of any length $N\ge N_0$ fall into two classed:</p>
<ol>
<li><p>A set whose total probability is less than $\epsilon$</p>
</li>
<li><p>$|\frac{\log p^{-1}}{N}-H|&lt; \delta$</p>
</li>
</ol>
<p>语言的冗余</p>
<h3 id="Representation-of-the-encoding-and-decoding-operations"><a href="#Representation-of-the-encoding-and-decoding-operations" class="headerlink" title="Representation of the encoding and decoding operations"></a>Representation of the encoding and decoding operations</h3><p>传感器：<br>$$<br>y_n&#x3D;f(x_n,\alpha_n)\<br>\alpha_{n+1}&#x3D;g(x_n,\alpha_n)<br>$$<br>$\alpha$表示传感器的状态</p>
<p>如果存在逆，则为非奇异</p>
<p><strong>Theorem</strong></p>
<p>传感器output的熵$\le$ input的熵，当且仅当非奇异时取等</p>
<p>然后讨论了通过适当的转移概率分配，信道上符号的熵可以最大化</p>
<p><strong>Theorem</strong></p>
<p>Source has entropy $H$, channel has capacity $C$. Then the average rate can be $\frac{C}{H}-\epsilon$, not greater than $\frac{C}{H}$.</p>
<p>不能超过这是因为每秒传输器传的熵等于源的熵，小于等于频道容量。</p>
</article></div></main><div class="nav-wrap"><div class="nav"><button class="site-nav"><div class="navicon"></div></button><ul class="nav_items"><li class="nav_item"><a class="nav-page" target="_blank" rel="noopener" href="https://scholar.google.com/citations?user=0tlXSPkAAAAJ&amp;hl=zh-CN"> Publications</a></li><li class="nav_item"><a class="nav-page" href="/blogs"> Blogs (in Chinese)</a></li></ul></div><div class="cd-top"><i class="fa fa-arrow-up" aria-hidden="true"></i></div></div><footer id="page_footer"><div class="footer_wrap"><div class="copyright">&copy;2021 - 2024 by Ruizhe Shi</div><div class="theme-info">Powered by <a target="_blank" href="https://hexo.io" rel="nofollow noopener">Hexo</a> & <a target="_blank" href="https://github.com/PhosphorW/hexo-theme-academia" rel="nofollow noopener">Academia Theme</a></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery-pjax@latest/jquery.pjax.min.js"></script><script src="/js/main.js"></script></body></html>